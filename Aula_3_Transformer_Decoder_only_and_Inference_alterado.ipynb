{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vitorms085/atividades-resolvidas-gsi073/blob/main/Aula_3_Transformer_Decoder_only_and_Inference_alterado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Aula 3 - Transformer *decoder-only*\n",
        "\n",
        "Nesta aula você irá modificar o Transformer *decoder-only* fornecido a seguir.\n",
        "Observe que em um *decoder-only* não existe:\n",
        "\n",
        "*   cross-attention\n",
        "*   encoder separado\n",
        "\n",
        "e é utilizado com *auto-regressão*.\n",
        "\n",
        "## Objetivo\n",
        "\n",
        "\n",
        "## Exercício\n",
        "\n",
        "Neste exercício você deve:\n",
        "\n",
        "1.   carregar o seu conjunto de documentos\n",
        "2.   treinar e usar (ou carregar) um tokenizador\n",
        "3.   fazer treino de um modelo decoder-only\n",
        "4.   incluir no loop de treino, inferência usando máxima probabilidade\n",
        "5.   incluir no loop de treino, inferência usando amostragem com temperatura\n"
      ],
      "metadata": {
        "id": "5WKD2sY0AuFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "aU6lT4FQAtGg"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Passo 1: carregar conjunto de documentos"
      ],
      "metadata": {
        "id": "ar5yEq-3H8oZ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNTLYlj6c3HH"
      },
      "source": [
        "from datasets import load_dataset\n",
        "import re\n",
        "import unicodedata # Importar unicodedata para remover acentos\n",
        "\n",
        "###### INSIRA AQUI O CODIGO PARA CARREGAR OS SEUS DOCUMENTOS na lista DOCUMENTOS\n",
        "with open('/content/frases.txt', 'r', encoding='utf-8') as f:\n",
        "    ds = f.readlines()\n",
        "\n",
        "# print(\"Esse é o conteúdo do arquivo:\")\n",
        "# print(ds)\n",
        "def clean_ascii(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace(\"é\", \"eh\")\n",
        "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
        "\n",
        "# Extract documentos directly from the list of lines (ds)\n",
        "documentos = [clean_ascii(x.strip()) for x in ds]\n",
        "# documentos = [t.split(\" - \")[0] for t in documentos]   # optional remove year\n",
        "documentos = [t for t in documentos if len(t) > 0]\n",
        "\n",
        "#documentos = [ \"meu doc favorito 1\", \"meu doc menos favorito 2\"]\n",
        "# print(\"Total documentos:\", len(documentos))\n",
        "# print(\"Sample:\", documentos[:10])"
      ],
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Passo 2: Carregar ou treinar um tokenizador"
      ],
      "metadata": {
        "id": "SuLZ15ODIFeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.trainers import WordLevelTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "import random\n",
        "import re\n",
        "\n",
        "##### Insira aqui o código para treinar o seu TOKENIZER\n",
        "# Defina o seu tokenizador\n",
        "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "trainer = WordLevelTrainer(\n",
        "    vocab_size=1000,\n",
        "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]\n",
        ")\n",
        "# Treino do tokenizador\n",
        "tokenizer.train_from_iterator(documentos, trainer)\n",
        "\n",
        "# Funções auxiliares para transitar entre tokens textuais e ids de tokens\n",
        "def encode(text):\n",
        "    ids = tokenizer.encode(\"[BOS] \" + text + \" [EOS]\").ids\n",
        "    return torch.tensor(ids, dtype=torch.long)\n",
        "\n",
        "def decode(ids):\n",
        "    return tokenizer.decode(ids.tolist())\n",
        "\n",
        "vocab_size = tokenizer.get_vocab_size()"
      ],
      "metadata": {
        "id": "7D2487loFG_Z"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definição do modelo Transformer Decoder-only"
      ],
      "metadata": {
        "id": "EUXZOhS2Ima9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderOnlyTransformer(nn.Module):\n",
        "    \"Implementação de um transformer que tem somente a parte do decoder\"\n",
        "    def __init__(self, vocab_size, d_model=128, n_heads=4, num_layers=3, max_len=64):\n",
        "        super().__init__()\n",
        "        self.max_len = max_len\n",
        "\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
        "\n",
        "        layer = nn.TransformerDecoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=256,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.decoder = nn.TransformerDecoder(layer, num_layers=num_layers)\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.shape\n",
        "        pos = torch.arange(T, device=x.device).unsqueeze(0)\n",
        "\n",
        "        h = self.token_emb(x) + self.pos_emb(pos)\n",
        "\n",
        "        # Máscara causal\n",
        "        mask = torch.triu(torch.ones(T, T, device=x.device), diagonal=1).bool()\n",
        "\n",
        "        out = self.decoder(h, h, tgt_mask=mask)\n",
        "        logits = self.lm_head(out)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "sZaswUVoDICQ"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Códigos de inferência"
      ],
      "metadata": {
        "id": "yxG3UcNjIub6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Código de inferência simples: token com maior probabilidade\n"
      ],
      "metadata": {
        "id": "0o_3Z8ZWF1fj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def max_prob_sampling(logits):\n",
        "    next_token = logits.argmax(dim=-1)\n",
        "    return next_token.unsqueeze(0)"
      ],
      "metadata": {
        "id": "DQP1EmACGBsF"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Código de inferência avançada: amostragem com temperatura"
      ],
      "metadata": {
        "id": "lqgdlkqHF6Gq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Inferência (com temperature e top_p)\n",
        "def sampling(logits, top_p=0.9, top_k=None, temperature=1.0):\n",
        "    # Ajusta pela temperatura\n",
        "    logits = logits / temperature\n",
        "\n",
        "    # Se top_k for especificado, filtra por top_k primeiro\n",
        "    if top_k is not None:\n",
        "        # Ensure top_k is not larger than the vocabulary size\n",
        "        k_to_use = min(top_k, logits.size(-1))\n",
        "        # Get the top_k values and indices\n",
        "        v, _ = torch.topk(logits, k_to_use)\n",
        "        # Set logits of all values smaller than the k-th value to -inf\n",
        "        logits[logits < v[:, [-1]]] = float('-inf')\n",
        "\n",
        "    # Ordena os logits\n",
        "    sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "    cumulative_probs = torch.softmax(sorted_logits, dim=-1).cumsum(dim=-1)\n",
        "\n",
        "    # Mascara tokens acima do top_p\n",
        "    mask = cumulative_probs > top_p\n",
        "    # Garante que ao menos um token permaneça\n",
        "    mask[..., 1:] = mask[..., :-1].clone()\n",
        "    mask[..., 0] = False\n",
        "\n",
        "    filtered_logits = sorted_logits.masked_fill(mask, float('-inf'))\n",
        "    probs = torch.softmax(filtered_logits, dim=-1)\n",
        "\n",
        "    # Amostra o token\n",
        "    sampled_idx = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "    # Converte para índice na tabela original\n",
        "    next_token = sorted_indices[sampled_idx]\n",
        "\n",
        "    return next_token\n",
        "\n",
        "\n",
        "def generate(prompt, next_token_function, max_new_tokens=2, top_k=None, top_p=0.9, temperature=1):\n",
        "    model.eval()\n",
        "    x = encode(prompt).unsqueeze(0).to(device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        logits = model(x)[:, -1, :]  # pega apenas o último passo\n",
        "\n",
        "        # Passa os parâmetros de amostragem se a função for top_p_sampling\n",
        "        if next_token_function == sampling:\n",
        "            next_token = next_token_function(logits.squeeze(0), top_k=top_k, top_p=top_p, temperature=temperature)\n",
        "        else:\n",
        "            next_token = next_token_function(logits.squeeze(0))\n",
        "\n",
        "        x = torch.cat([x, next_token.unsqueeze(0)], dim=1)\n",
        "\n",
        "        if next_token.item() == tokenizer.token_to_id(\"[EOS]\"):\n",
        "            break\n",
        "\n",
        "    return decode(x[0])\n"
      ],
      "metadata": {
        "id": "pGvot4bvd3il"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Fazer treino de modelo: códigos de treino"
      ],
      "metadata": {
        "id": "yFyl_KB-HAak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função auxiliar para gerar batches de exemplos para treino\n",
        "def sample_batch(batch_size=16, max_len=20):\n",
        "    batch = random.sample(documentos, batch_size)\n",
        "    tokenized = [encode(t) for t in batch]\n",
        "\n",
        "    max_t = min(max(len(x) for x in tokenized), max_len)\n",
        "    padded = []\n",
        "\n",
        "    for x in tokenized:\n",
        "        x = x[:max_t]\n",
        "        pad_len = max_t - len(x)\n",
        "        if pad_len > 0:\n",
        "            x = torch.cat([x, torch.zeros(pad_len, dtype=torch.long)])\n",
        "        padded.append(x)\n",
        "\n",
        "    return torch.stack(padded)\n",
        "\n",
        "################################\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = DecoderOnlyTransformer(vocab_size).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "#################################\n",
        "\n",
        "steps = 10000\n",
        "\n",
        "for step in range(1, steps + 1):\n",
        "    model.train()\n",
        "    batch = sample_batch().to(device)\n",
        "\n",
        "    logits = model(batch[:, :-1])\n",
        "    loss = F.cross_entropy(\n",
        "        logits.reshape(-1, vocab_size),\n",
        "        batch[:, 1:].reshape(-1),\n",
        "        ignore_index=tokenizer.token_to_id(\"[PAD]\")\n",
        "    )\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 50 == 0:\n",
        "        ppl = torch.exp(loss).item()\n",
        "        print(f\"[step {step}] loss={loss.item():.4f}, ppl={ppl:.2f}\")\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(\"Generated text:\")\n",
        "        print(generate(\"deus eh\", next_token_function=sampling ))\n",
        "        print(\"--------------------------------------\")\n",
        "\n",
        "print(\"Training completed.\")"
      ],
      "metadata": {
        "id": "_fMNjKHeds_q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 981
        },
        "outputId": "0d9ebc98-a049-4543-f5bf-f378269e9cbd"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[step 50] loss=6.7401, ppl=845.68\n",
            "[step 100] loss=6.5280, ppl=684.00\n",
            "Generated text:\n",
            "deus eh eficaz viventes\n",
            "--------------------------------------\n",
            "[step 150] loss=6.3417, ppl=567.78\n",
            "[step 200] loss=6.1275, ppl=458.31\n",
            "Generated text:\n",
            "deus eh outros abraao\n",
            "--------------------------------------\n",
            "[step 250] loss=5.9805, ppl=395.62\n",
            "[step 300] loss=5.7564, ppl=316.21\n",
            "Generated text:\n",
            "deus eh ampara sumo\n",
            "--------------------------------------\n",
            "[step 350] loss=5.6996, ppl=298.76\n",
            "[step 400] loss=5.4355, ppl=229.40\n",
            "Generated text:\n",
            "deus eh segundo como\n",
            "--------------------------------------\n",
            "[step 450] loss=5.4761, ppl=238.92\n",
            "[step 500] loss=5.4063, ppl=222.81\n",
            "Generated text:\n",
            "deus eh louvor\n",
            "--------------------------------------\n",
            "[step 550] loss=5.3358, ppl=207.64\n",
            "[step 600] loss=5.2250, ppl=185.87\n",
            "Generated text:\n",
            "deus eh desde espada\n",
            "--------------------------------------\n",
            "[step 650] loss=4.8543, ppl=128.30\n",
            "[step 700] loss=5.2757, ppl=195.52\n",
            "Generated text:\n",
            "deus eh sal livres\n",
            "--------------------------------------\n",
            "[step 750] loss=5.0123, ppl=150.24\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1682969127.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1682969127.py\u001b[0m in \u001b[0;36msample_batch\u001b[0;34m(batch_size, max_len)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msample_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocumentos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtokenized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmax_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4262535956.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Funções auxiliares para transitar entre tokens textuais e ids de tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[BOS] \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" [EOS]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercício: controle de temperatura, Top-K e Top-P\n",
        "\n",
        "Modifique o código a seguir para fazer a visualização de produção de tokens do modelo que você treinou."
      ],
      "metadata": {
        "id": "UHqvaoOWJ2yJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# -----------------------------\n",
        "# Mock language-model logits\n",
        "# -----------------------------\n",
        "TOKENS = [\"blue\", \"purple\", \"violet\", \"vio\", \"not\", \"Blue\", \"green\", \"gray\", \"grey\", \"black\"]\n",
        "BASE_LOGITS = np.array([6.0, 1.5, 0.5, 0.2, 0.2, 0.0, -5.0, -5.0, -5.0, -5.0])\n",
        "\n",
        "def softmax(x):\n",
        "    e = np.exp(x - np.max(x))\n",
        "    return e / e.sum()\n",
        "\n",
        "def apply_temperature(logits, temperature):\n",
        "    return logits / max(temperature, 1e-5)\n",
        "\n",
        "def apply_top_k(probs, k):\n",
        "    if k >= len(probs):\n",
        "        return probs\n",
        "    idx = np.argsort(probs)[::-1]\n",
        "    mask = np.zeros_like(probs)\n",
        "    mask[idx[:k]] = 1\n",
        "    probs = probs * mask\n",
        "    return probs / probs.sum()\n",
        "\n",
        "def apply_top_p(probs, p):\n",
        "    idx = np.argsort(probs)[::-1]\n",
        "    cumulative = np.cumsum(probs[idx])\n",
        "    mask = cumulative <= p\n",
        "    mask[np.argmax(mask)] = True\n",
        "    new_probs = np.zeros_like(probs)\n",
        "    new_probs[idx[mask]] = probs[idx[mask]]\n",
        "    return new_probs / new_probs.sum()\n",
        "\n",
        "# -----------------------------\n",
        "# Widgets\n",
        "# -----------------------------\n",
        "prompt_dropdown = widgets.Dropdown(\n",
        "    options=[\"Roses are red, violets are...\"],\n",
        "    value=\"Roses are red, violets are...\",\n",
        "    description=\"Prompt\",\n",
        "    layout=widgets.Layout(width=\"95%\")\n",
        ")\n",
        "\n",
        "temperature_slider = widgets.FloatSlider(\n",
        "    value=1.0, min=0.1, max=5.0, step=0.1,\n",
        "    description=\"Temperatura\",\n",
        "    readout_format=\".1f\",\n",
        "    layout=widgets.Layout(width=\"90%\")\n",
        ")\n",
        "\n",
        "topk_slider = widgets.IntSlider(\n",
        "    value=6, min=1, max=10, step=1,\n",
        "    description=\"Top-K\",\n",
        "    layout=widgets.Layout(width=\"90%\")\n",
        ")\n",
        "\n",
        "topp_slider = widgets.FloatSlider(\n",
        "    value=1.0, min=0.1, max=1.0, step=0.05,\n",
        "    description=\"Top-P\",\n",
        "    readout_format=\".2f\",\n",
        "    layout=widgets.Layout(width=\"90%\")\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Plot\n",
        "# -----------------------------\n",
        "output_plot = widgets.Output()\n",
        "\n",
        "def update_plot(*args):\n",
        "    with output_plot:\n",
        "        output_plot.clear_output()\n",
        "\n",
        "        logits = apply_temperature(BASE_LOGITS, temperature_slider.value)\n",
        "        probs = softmax(logits)\n",
        "        probs = apply_top_k(probs, topk_slider.value)\n",
        "        probs = apply_top_p(probs, topp_slider.value)\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(10, 4))\n",
        "        bars = ax.bar(TOKENS, probs * 100)\n",
        "        ax.set_ylim(0, 100)\n",
        "        ax.set_ylabel(\"Probabilidade (%)\")\n",
        "        ax.set_title(\"Probabilidade do próximo token\")\n",
        "\n",
        "        for bar, p in zip(bars, probs):\n",
        "            ax.text(\n",
        "                bar.get_x() + bar.get_width() / 2,\n",
        "                bar.get_height(),\n",
        "                f\"{p*100:.2f}%\",\n",
        "                ha=\"center\",\n",
        "                va=\"bottom\",\n",
        "                fontsize=9\n",
        "            )\n",
        "\n",
        "        plt.xticks(rotation=0)\n",
        "        plt.show()\n",
        "\n",
        "for w in [temperature_slider, topk_slider, topp_slider]:\n",
        "    w.observe(update_plot, names=\"value\")\n",
        "\n",
        "# -----------------------------\n",
        "# Collapsible explanation\n",
        "# -----------------------------\n",
        "accordion = widgets.Accordion(\n",
        "    children=[widgets.HTML(\n",
        "        \"\"\"\n",
        "        <ul>\n",
        "          <li><b>Temperatura</b>: Controla aleatoriedade com mudança na escala dos logits.</li>\n",
        "          <li><b>Top-K</b>: Restringe a amostra aos K tokens mais prováveis.</li>\n",
        "          <li><b>Top-P</b>: Usa somente os menor conjunto de tokens que resultam em probabilidade acumulada até P.</li>\n",
        "        </ul>\n",
        "        \"\"\"\n",
        "    )]\n",
        ")\n",
        "accordion.set_title(0, \"Entenda a visualização\")\n",
        "\n",
        "# -----------------------------\n",
        "# Layout\n",
        "# -----------------------------\n",
        "controls = widgets.VBox([\n",
        "    prompt_dropdown,\n",
        "    widgets.HBox([temperature_slider]),\n",
        "    widgets.HBox([topk_slider]),\n",
        "    widgets.HBox([topp_slider])\n",
        "])\n",
        "\n",
        "display(Markdown(\"## Visualização de controle de Temperatura, Top-K e Top-P.\"))\n",
        "display(widgets.VBox([\n",
        "    widgets.HTML(\"<b>Parameters</b>\"),\n",
        "    controls,\n",
        "    output_plot,\n",
        "    accordion\n",
        "]))\n",
        "\n",
        "update_plot()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-3PHgy-JH6hM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}